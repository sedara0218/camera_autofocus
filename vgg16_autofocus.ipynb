{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"always\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np, matplotlib.pyplot as plt\n",
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nLOAD PRE-TRAINED MODEL\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "LOAD PRE-TRAINED MODEL\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications import VGG16\n",
    "\n",
    "#Load the VGG model\n",
    "vgg16 = VGG16(include_top=False, input_shape=(224, 224, 3)) # load vgg16 with all defaults except top"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nMARK ALL LAYERS AS NON-TRAINABLE SO TRAINING DOESN'T ALTER WEIGHTS\\n\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "MARK ALL LAYERS AS NON-TRAINABLE SO TRAINING DOESN'T ALTER WEIGHTS\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in vgg16.layers:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 224, 224, 3)       0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n",
      "=================================================================\n",
      "Total params: 14,714,688\n",
      "Trainable params: 0\n",
      "Non-trainable params: 14,714,688\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print vgg16.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nTUNE MODEL TO AUTOFOCUS NEEDS\\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "TUNE MODEL TO AUTOFOCUS NEEDS\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 224, 224, 3)       0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d_1 ( (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 2)                 1026      \n",
      "=================================================================\n",
      "Total params: 14,715,714\n",
      "Trainable params: 1,026\n",
      "Non-trainable params: 14,714,688\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras import layers\n",
    "\n",
    "# my_model_output = vgg16.layers[10].output # Only use the first three blocks of convolution\n",
    "my_model_output = vgg16.output # use all 5 conv layers of vgg\n",
    "my_model_output = layers.GlobalAveragePooling2D()(my_model_output) # Then add a GlobalAveragePooling layer to get 1d representation\n",
    "# my_model_output = layers.Dense(128, activation='relu')(my_model_output) # add a Dense layer just to increase trainable params\n",
    "# my_model_output = layers.Dropout(0.5)(my_model_output) # Add regularization to help decrease overfitting data\n",
    "my_model_output = layers.Dense(2, activation='softmax')(my_model_output) # binary classification prediction layer\n",
    "\n",
    "my_model = keras.models.Model(inputs=vgg16.input, outputs=my_model_output) #created my neural network\n",
    "\n",
    "my_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nSET UP TRAINING AND VALIDATION DATA\\n'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "SET UP TRAINING AND VALIDATION DATA\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1400 images belonging to 2 classes.\n",
      "Found 600 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator, load_img\n",
    "\n",
    "train_datagen = ImageDataGenerator(\n",
    "                            rescale=1./255,\n",
    "                            rotation_range=20,\n",
    "                            width_shift_range=0.2,\n",
    "                            height_shift_range=0.2,\n",
    "                            horizontal_flip=True,\n",
    "                            vertical_flip=True,\n",
    "                            fill_mode='nearest')\n",
    "\n",
    "validation_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "train_batchsize = 20\n",
    "val_batchsize = 5\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "                                                'train_data',\n",
    "                                                target_size=(224, 224),\n",
    "                                                batch_size=train_batchsize,\n",
    "                                                class_mode='categorical')\n",
    "\n",
    "validation_generator = validation_datagen.flow_from_directory(\n",
    "                                                    'validation_data',\n",
    "                                                    target_size=(224, 224),\n",
    "                                                    batch_size=val_batchsize,\n",
    "                                                    class_mode='categorical',\n",
    "                                                    shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nCOMPILE AND TRAIN MODEL\\n'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "COMPILE AND TRAIN MODEL\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training new model...\n",
      "Epoch 1/25\n",
      "140/140 [==============================] - 1348s 10s/step - loss: 0.6583 - acc: 0.6286 - val_loss: 0.6194 - val_acc: 0.7017\n",
      "\n",
      "Epoch 00001: saving model to autofocus_vgg16.h5\n",
      "Epoch 2/25\n",
      "140/140 [==============================] - 1346s 10s/step - loss: 0.6095 - acc: 0.6796 - val_loss: 0.5831 - val_acc: 0.7200\n",
      "\n",
      "Epoch 00002: saving model to autofocus_vgg16.h5\n",
      "Epoch 3/25\n",
      "140/140 [==============================] - 1353s 10s/step - loss: 0.5816 - acc: 0.7075 - val_loss: 0.5773 - val_acc: 0.7117\n",
      "\n",
      "Epoch 00003: saving model to autofocus_vgg16.h5\n",
      "Epoch 4/25\n",
      "140/140 [==============================] - 1350s 10s/step - loss: 0.5634 - acc: 0.7214 - val_loss: 0.5445 - val_acc: 0.7367\n",
      "\n",
      "Epoch 00004: saving model to autofocus_vgg16.h5\n",
      "Epoch 5/25\n",
      "140/140 [==============================] - 1349s 10s/step - loss: 0.5577 - acc: 0.7300 - val_loss: 0.5307 - val_acc: 0.7450\n",
      "\n",
      "Epoch 00005: saving model to autofocus_vgg16.h5\n",
      "Epoch 6/25\n",
      "140/140 [==============================] - 1348s 10s/step - loss: 0.5400 - acc: 0.7357 - val_loss: 0.5223 - val_acc: 0.7450\n",
      "\n",
      "Epoch 00006: saving model to autofocus_vgg16.h5\n",
      "Epoch 7/25\n",
      "140/140 [==============================] - 1356s 10s/step - loss: 0.5297 - acc: 0.7357 - val_loss: 0.5227 - val_acc: 0.7417\n",
      "\n",
      "Epoch 00007: saving model to autofocus_vgg16.h5\n",
      "Epoch 8/25\n",
      "140/140 [==============================] - 1386s 10s/step - loss: 0.5283 - acc: 0.7461 - val_loss: 0.5042 - val_acc: 0.7633\n",
      "\n",
      "Epoch 00008: saving model to autofocus_vgg16.h5\n",
      "Epoch 9/25\n",
      "140/140 [==============================] - 1503s 11s/step - loss: 0.5213 - acc: 0.7482 - val_loss: 0.5012 - val_acc: 0.7600\n",
      "\n",
      "Epoch 00009: saving model to autofocus_vgg16.h5\n",
      "Epoch 10/25\n",
      "140/140 [==============================] - 1455s 10s/step - loss: 0.5168 - acc: 0.7471 - val_loss: 0.5151 - val_acc: 0.7417\n",
      "\n",
      "Epoch 00010: saving model to autofocus_vgg16.h5\n",
      "Epoch 11/25\n",
      "140/140 [==============================] - 1457s 10s/step - loss: 0.5160 - acc: 0.7511 - val_loss: 0.4921 - val_acc: 0.7600\n",
      "\n",
      "Epoch 00011: saving model to autofocus_vgg16.h5\n",
      "Epoch 12/25\n",
      "140/140 [==============================] - 1406s 10s/step - loss: 0.5096 - acc: 0.7461 - val_loss: 0.4987 - val_acc: 0.7533\n",
      "\n",
      "Epoch 00012: saving model to autofocus_vgg16.h5\n",
      "Epoch 13/25\n",
      "140/140 [==============================] - 1381s 10s/step - loss: 0.5092 - acc: 0.7554 - val_loss: 0.4838 - val_acc: 0.7650\n",
      "\n",
      "Epoch 00013: saving model to autofocus_vgg16.h5\n",
      "Epoch 14/25\n",
      "140/140 [==============================] - 1486s 11s/step - loss: 0.5022 - acc: 0.7604 - val_loss: 0.4980 - val_acc: 0.7567\n",
      "\n",
      "Epoch 00014: saving model to autofocus_vgg16.h5\n",
      "Epoch 15/25\n",
      "140/140 [==============================] - 1617s 12s/step - loss: 0.5024 - acc: 0.7589 - val_loss: 0.4975 - val_acc: 0.7533\n",
      "\n",
      "Epoch 00015: saving model to autofocus_vgg16.h5\n",
      "Epoch 16/25\n",
      "140/140 [==============================] - 1510s 11s/step - loss: 0.4979 - acc: 0.7643 - val_loss: 0.4946 - val_acc: 0.7567\n",
      "\n",
      "Epoch 00016: saving model to autofocus_vgg16.h5\n",
      "Epoch 17/25\n",
      "140/140 [==============================] - 1469s 10s/step - loss: 0.4925 - acc: 0.7600 - val_loss: 0.4974 - val_acc: 0.7600\n",
      "\n",
      "Epoch 00017: saving model to autofocus_vgg16.h5\n",
      "Epoch 18/25\n",
      "140/140 [==============================] - 36601s 261s/step - loss: 0.4916 - acc: 0.7668 - val_loss: 0.4811 - val_acc: 0.7633\n",
      "\n",
      "Epoch 00018: saving model to autofocus_vgg16.h5\n",
      "Epoch 19/25\n",
      "140/140 [==============================] - 1344s 10s/step - loss: 0.4886 - acc: 0.7704 - val_loss: 0.5058 - val_acc: 0.7483\n",
      "\n",
      "Epoch 00019: saving model to autofocus_vgg16.h5\n",
      "Epoch 20/25\n",
      "140/140 [==============================] - 1341s 10s/step - loss: 0.4888 - acc: 0.7696 - val_loss: 0.4911 - val_acc: 0.7567\n",
      "\n",
      "Epoch 00020: saving model to autofocus_vgg16.h5\n",
      "Epoch 21/25\n",
      "140/140 [==============================] - 1441s 10s/step - loss: 0.4835 - acc: 0.7596 - val_loss: 0.4945 - val_acc: 0.7583\n",
      "\n",
      "Epoch 00021: saving model to autofocus_vgg16.h5\n",
      "Epoch 22/25\n",
      "140/140 [==============================] - 1451s 10s/step - loss: 0.4887 - acc: 0.7632 - val_loss: 0.4772 - val_acc: 0.7633\n",
      "\n",
      "Epoch 00022: saving model to autofocus_vgg16.h5\n",
      "Epoch 23/25\n",
      "140/140 [==============================] - 1434s 10s/step - loss: 0.4934 - acc: 0.7661 - val_loss: 0.4809 - val_acc: 0.7683\n",
      "\n",
      "Epoch 00023: saving model to autofocus_vgg16.h5\n",
      "Epoch 24/25\n",
      "140/140 [==============================] - 1381s 10s/step - loss: 0.4784 - acc: 0.7671 - val_loss: 0.4759 - val_acc: 0.7633\n",
      "\n",
      "Epoch 00024: saving model to autofocus_vgg16.h5\n",
      "Epoch 25/25\n",
      "140/140 [==============================] - 1345s 10s/step - loss: 0.4792 - acc: 0.7761 - val_loss: 0.4912 - val_acc: 0.7633\n",
      "\n",
      "Epoch 00025: saving model to autofocus_vgg16.h5\n"
     ]
    }
   ],
   "source": [
    "# Compile\n",
    "try:\n",
    "    my_model = keras.models.load_model('autofocus_vgg16.h5') # trains from last epoch if model exists\n",
    "except:\n",
    "    print \"Training new model...\"\n",
    "    my_model.compile(loss='categorical_crossentropy',\n",
    "                    optimizer='adam',\n",
    "                    metrics=['acc'])\n",
    "\n",
    "# Train\n",
    "checkpoint = keras.callbacks.ModelCheckpoint('autofocus_vgg16.h5', monitor='val_loss', verbose=1, period=1)\n",
    "\n",
    "my_model_hist = my_model.fit_generator(\n",
    "                            train_generator,\n",
    "                            steps_per_epoch=2*train_generator.samples/train_generator.batch_size ,\n",
    "                            epochs=25,\n",
    "                            validation_data=validation_generator,\n",
    "                            validation_steps=validation_generator.samples/validation_generator.batch_size,\n",
    "                            verbose=1,\n",
    "                            callbacks=[checkpoint])\n",
    "\n",
    "# Save the Model\n",
    "my_model.save('autofocus_vgg16.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nPREDICT\\n'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "PREDICT\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import keras\n",
    "from keras.models import load_model\n",
    "from keras.preprocessing import image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.load_model('autofocus_vgg16.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image_as_tensor(img_path):\n",
    "    img = image.load_img(img_path, target_size=(224, 224))\n",
    "    img = image.img_to_array(img)\n",
    "    img = np.expand_dims(img, axis=0)\n",
    "    img /= 255.\n",
    "    return img\n",
    "\n",
    "def predict(img_path):\n",
    "    img = load_image_as_tensor(img_path)\n",
    "    prob = model.predict(img)\n",
    "    classes = np.argmax(prob, axis=1)\n",
    "    label_map = (validation_generator.class_indices)\n",
    "    print img_path\n",
    "    print prob, classes, label_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_blur/black_square.jpg\n",
      "[[0.54110146 0.4588986 ]] [0] {'focused': 0, 'unfocused': 1}\n",
      "test_blur/black_square9.jpg\n",
      "[[0.46069276 0.53930724]] [1] {'focused': 0, 'unfocused': 1}\n",
      "test_blur/black_square17.jpg\n",
      "[[0.35018128 0.6498188 ]] [1] {'focused': 0, 'unfocused': 1}\n",
      "test_blur/black_square25.jpg\n",
      "[[0.29431987 0.7056802 ]] [1] {'focused': 0, 'unfocused': 1}\n",
      "test_blur/black_square33.jpg\n",
      "[[0.2458293  0.75417066]] [1] {'focused': 0, 'unfocused': 1}\n",
      "test_blur/black_square41.jpg\n",
      "[[0.22120003 0.77879995]] [1] {'focused': 0, 'unfocused': 1}\n"
     ]
    }
   ],
   "source": [
    "predict('test_blur/black_square.jpg') \n",
    "predict('test_blur/black_square9.jpg')\n",
    "predict('test_blur/black_square17.jpg')\n",
    "predict('test_blur/black_square25.jpg')\n",
    "predict('test_blur/black_square33.jpg')\n",
    "predict('test_blur/black_square41.jpg')\n",
    "\n",
    "# 100% Accurate for these 6 test images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
